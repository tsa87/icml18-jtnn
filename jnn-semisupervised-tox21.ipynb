{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from utils import time_since\n",
    "import traceback\n",
    "\n",
    "import time\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math, random\n",
    "import argparse\n",
    "from collections import deque\n",
    "import cPickle as pickle\n",
    "\n",
    "from fast_jtnn import *\n",
    "import rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = 10 + 1 #add one because first col is smile\n",
    "target_unlabeled_percentage = 0.95\n",
    "early_stop_thresh = 2\n",
    "\n",
    "load_epoch = 0\n",
    "alpha = 100 #0.1 * len_unlabelled / len_labelled\n",
    "\n",
    "hidden_size=56 #450\n",
    "batch_size=8\n",
    "latent_size=28\n",
    "depthT=20\n",
    "depthG=3\n",
    "y_size=2\n",
    "\n",
    "lr=1e-3\n",
    "clip_norm=50.0\n",
    "beta=0.0\n",
    "step_beta=0.002\n",
    "max_beta=1.0\n",
    "warmup=40 #40000\n",
    "\n",
    "epoch=5\n",
    "test_epoch=1\n",
    "anneal_rate=0.9\n",
    "anneal_iter=40 #40000\n",
    "kl_anneal_iter=20 #2000\n",
    "\n",
    "save_iter=50\n",
    "print_iter=5\n",
    "\n",
    "num_workers = 4\n",
    "has_cuda = torch.cuda.is_available()\n",
    "\n",
    "save_dir = 'data/tox21/model'\n",
    "train_folder = 'fast_molvae/moses-processed/quick_train'\n",
    "test_folder = 'fast_molvae/moses-processed/quick_test'\n",
    "vocab_file = 'data/tox21/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [x.strip(\"\\r\\n \") for x in open(vocab_file)]\n",
    "vocab = Vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsa87/miniconda3/envs/python2/lib/python2.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemiJTNNVAEClassifier(\n",
      "  (jtnn): JTNNEncoder(\n",
      "    (embedding): Embedding(550, 56)\n",
      "    (outputNN): Sequential(\n",
      "      (0): Linear(in_features=112, out_features=56, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (GRU): GraphGRU(\n",
      "      (W_z): Linear(in_features=112, out_features=56, bias=True)\n",
      "      (W_r): Linear(in_features=56, out_features=56, bias=False)\n",
      "      (U_r): Linear(in_features=56, out_features=56, bias=True)\n",
      "      (W_h): Linear(in_features=112, out_features=56, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): JTNNDecoder(\n",
      "    (embedding): Embedding(550, 56)\n",
      "    (W_z): Linear(in_features=112, out_features=56, bias=True)\n",
      "    (U_r): Linear(in_features=56, out_features=56, bias=False)\n",
      "    (W_r): Linear(in_features=56, out_features=56, bias=True)\n",
      "    (W_h): Linear(in_features=112, out_features=56, bias=True)\n",
      "    (W): Linear(in_features=70, out_features=56, bias=True)\n",
      "    (U): Linear(in_features=70, out_features=56, bias=True)\n",
      "    (U_i): Linear(in_features=112, out_features=56, bias=True)\n",
      "    (W_o): Linear(in_features=56, out_features=550, bias=True)\n",
      "    (U_o): Linear(in_features=56, out_features=1, bias=True)\n",
      "    (pred_loss): CrossEntropyLoss()\n",
      "    (stop_loss): BCEWithLogitsLoss()\n",
      "  )\n",
      "  (jtmpn): JTMPN(\n",
      "    (W_i): Linear(in_features=40, out_features=56, bias=False)\n",
      "    (W_h): Linear(in_features=56, out_features=56, bias=False)\n",
      "    (W_o): Linear(in_features=91, out_features=56, bias=True)\n",
      "  )\n",
      "  (mpn): MPN(\n",
      "    (W_i): Linear(in_features=50, out_features=56, bias=False)\n",
      "    (W_h): Linear(in_features=56, out_features=56, bias=False)\n",
      "    (W_o): Linear(in_features=95, out_features=56, bias=True)\n",
      "  )\n",
      "  (A_assm): Linear(in_features=14, out_features=56, bias=False)\n",
      "  (assm_loss): CrossEntropyLoss()\n",
      "  (T_mean): Linear(in_features=58, out_features=14, bias=True)\n",
      "  (T_var): Linear(in_features=58, out_features=14, bias=True)\n",
      "  (G_mean): Linear(in_features=58, out_features=14, bias=True)\n",
      "  (G_var): Linear(in_features=58, out_features=14, bias=True)\n",
      "  (predictor): Sequential(\n",
      "    (0): Linear(in_features=112, out_features=56, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=56, out_features=2, bias=True)\n",
      "    (3): Softmax(dim=None)\n",
      "  )\n",
      "  (pred_loss): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "weight = None #torch.tensor([1., 5.])\n",
    "\n",
    "model = SemiJTNNVAEClassifier(vocab, hidden_size, latent_size, y_size, depthT, depthG, alpha, weight=weight)\n",
    "if has_cuda: model = model.cuda()\n",
    "print model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    if param.dim() == 1:\n",
    "        nn.init.constant_(param, 0)\n",
    "    else:\n",
    "        nn.init.xavier_normal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #Params: 183K\n"
     ]
    }
   ],
   "source": [
    "if load_epoch > 0:\n",
    "    model.load_state_dict(torch.load(save_dir + \"/model.iter-\" + str(load_epoch)))\n",
    "    \n",
    "print \"Model #Params: %dK\" % (sum([x.nelement() for x in model.parameters()]) / 1000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, anneal_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_norm = lambda m: math.sqrt(sum([p.norm().item() ** 2 for p in m.parameters()]))\n",
    "grad_norm = lambda m: math.sqrt(sum([p.grad.norm().item() ** 2 for p in m.parameters() if p.grad is not None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing this because 20% of labelled data got lost in test\n",
    "tracker = IndexTracker(train_folder, label_idx=11, label_pct=(1-target_unlabeled_percentage)/0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsa87/miniconda3/envs/python2/lib/python2.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/tsa87/miniconda3/envs/python2/lib/python2.7/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/tsa87/miniconda3/envs/python2/lib/python2.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 72, in <module>\n",
      "    unsupervised_loss, _, kl_div1, wacc1, tacc1, sacc1, _, _ = model(unsupervised_batch['data'], None, beta)\n",
      "  File \"/home/tsa87/miniconda3/envs/python2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"fast_jtnn/jtnn_vae.py\", line 341, in forward\n",
      "    self._compute_partial_loss(y_batch, x_batch, x_tree_vecs, x_tree_mess, x_mol_vecs, x_jtmpn_holder, beta)\n",
      "  File \"fast_jtnn/jtnn_vae.py\", line 276, in _compute_partial_loss\n",
      "    word_loss, topo_loss, word_acc, topo_acc = self.decoder(x_batch, z_tree_vecs)\n",
      "  File \"/home/tsa87/miniconda3/envs/python2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"fast_jtnn/jtnn_dec.py\", line 61, in forward\n",
      "    dfs(s, mol_tree.nodes[0], -1)\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iter: 5\n",
      "[Train] Loss: 229.737, Clsf_loss: 65.68, KL: 5.02, Word: 0.09, Topo: 52.33, Assm: 50.98, Clsf: 68.75\n",
      "Epoch: 0 | Iter: 10\n",
      "[Train] Loss: 241.237, Clsf_loss: 57.53, KL: 6.68, Word: 0.32, Topo: 52.96, Assm: 43.35, Clsf: 80.00\n",
      "Epoch: 0 | Iter: 15\n",
      "[Train] Loss: 237.706, Clsf_loss: 51.88, KL: 10.26, Word: 5.42, Topo: 54.29, Assm: 58.52, Clsf: 82.50\n",
      "Epoch: 0 | Iter: 20\n",
      "[Train] Loss: 236.116, Clsf_loss: 50.05, KL: 14.76, Word: 10.77, Topo: 59.28, Assm: 60.20, Clsf: 82.50\n",
      "Epoch: 0 | Iter: 25\n",
      "[Train] Loss: 232.209, Clsf_loss: 47.34, KL: 18.24, Word: 12.72, Topo: 60.94, Assm: 56.67, Clsf: 85.00\n",
      "Epoch: 0 | Iter: 30\n",
      "[Train] Loss: 231.531, Clsf_loss: 45.76, KL: 19.29, Word: 15.05, Topo: 62.27, Assm: 59.62, Clsf: 87.50\n",
      "Epoch: 0 | Iter: 35\n",
      "[Train] Loss: 200.054, Clsf_loss: 50.68, KL: 19.40, Word: 16.04, Topo: 67.93, Assm: 66.59, Clsf: 82.50\n",
      "learning rate: 0.000900\n",
      "Epoch: 0 | Iter: 40\n",
      "[Train] Loss: 178.385, Clsf_loss: 51.89, KL: 20.33, Word: 18.70, Topo: 68.53, Assm: 69.73, Clsf: 80.00\n",
      "Epoch: 0 | Iter: 45\n",
      "[Train] Loss: 183.890, Clsf_loss: 49.56, KL: 22.60, Word: 18.54, Topo: 69.72, Assm: 68.91, Clsf: 82.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 72, in <module>\n",
      "    unsupervised_loss, _, kl_div1, wacc1, tacc1, sacc1, _, _ = model(unsupervised_batch['data'], None, beta)\n",
      "  File \"/home/tsa87/miniconda3/envs/python2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"fast_jtnn/jtnn_vae.py\", line 341, in forward\n",
      "    self._compute_partial_loss(y_batch, x_batch, x_tree_vecs, x_tree_mess, x_mol_vecs, x_jtmpn_holder, beta)\n",
      "  File \"fast_jtnn/jtnn_vae.py\", line 276, in _compute_partial_loss\n",
      "    word_loss, topo_loss, word_acc, topo_acc = self.decoder(x_batch, z_tree_vecs)\n",
      "  File \"/home/tsa87/miniconda3/envs/python2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"fast_jtnn/jtnn_dec.py\", line 108, in forward\n",
      "    cur_h_nei = torch.stack(cur_h_nei, dim=0).view(-1,MAX_NB,self.hidden_size)\n",
      "RuntimeError: shape '[-1, 15, 56]' is invalid for input of size 896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to data/tox21/model/model.iter-at step: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 93, in <module>\n",
      "    torch.save(model.state_dict(), save_dir + \"/model.iter-\" + str(total_step))\n",
      "  File \"/home/tsa87/miniconda3/envs/python2/lib/python2.7/site-packages/torch/serialization.py\", line 260, in save\n",
      "    return _with_file_like(f, \"wb\", lambda f: _save(obj, f, pickle_module, pickle_protocol))\n",
      "  File \"/home/tsa87/miniconda3/envs/python2/lib/python2.7/site-packages/torch/serialization.py\", line 183, in _with_file_like\n",
      "    f = open(f, mode)\n",
      "IOError: [Errno 2] No such file or directory: 'data/tox21/model/model.iter-50'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iter: 55\n",
      "[Train] Loss: 155.421, Clsf_loss: 47.02, KL: 27.81, Word: 27.28, Topo: 72.18, Assm: 67.38, Clsf: 84.72\n",
      "Epoch: 0 | Iter: 60\n",
      "[Train] Loss: 145.795, Clsf_loss: 44.82, KL: 42.24, Word: 30.25, Topo: 74.33, Assm: 68.83, Clsf: 90.00\n",
      "Epoch: 0 | Iter: 65\n",
      "[Train] Loss: 136.747, Clsf_loss: 45.96, KL: 50.12, Word: 28.89, Topo: 75.78, Assm: 76.48, Clsf: 87.50\n",
      "Epoch: 0 | Iter: 70\n",
      "[Train] Loss: 141.088, Clsf_loss: 45.49, KL: 69.49, Word: 29.35, Topo: 77.89, Assm: 76.26, Clsf: 87.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 72, in <module>\n",
      "    unsupervised_loss, _, kl_div1, wacc1, tacc1, sacc1, _, _ = model(unsupervised_batch['data'], None, beta)\n",
      "  File \"/home/tsa87/miniconda3/envs/python2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"fast_jtnn/jtnn_vae.py\", line 341, in forward\n",
      "    self._compute_partial_loss(y_batch, x_batch, x_tree_vecs, x_tree_mess, x_mol_vecs, x_jtmpn_holder, beta)\n",
      "  File \"fast_jtnn/jtnn_vae.py\", line 276, in _compute_partial_loss\n",
      "    word_loss, topo_loss, word_acc, topo_acc = self.decoder(x_batch, z_tree_vecs)\n",
      "  File \"/home/tsa87/miniconda3/envs/python2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"fast_jtnn/jtnn_dec.py\", line 108, in forward\n",
      "    cur_h_nei = torch.stack(cur_h_nei, dim=0).view(-1,MAX_NB,self.hidden_size)\n",
      "RuntimeError: shape '[-1, 15, 56]' is invalid for input of size 896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"fast_jtnn/datautils.py\", line 217, in __getitem__\n",
      "    'data': tensorize(self.data[idx], self.vocab, assm=self.assm),\n",
      "  File \"fast_jtnn/datautils.py\", line 244, in tensorize\n",
      "    jtmpn_holder = JTMPN.tensorize(cands, mess_dict)\n",
      "  File \"fast_jtnn/jtmpn.py\", line 122, in tensorize\n",
      "    fatoms = torch.stack(fatoms, 0)\n",
      "RuntimeError: stack expects a non-empty TensorList\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 0.000810\n",
      "Epoch: 0 | Iter: 80\n",
      "[Train] Loss: 137.836, Clsf_loss: 42.47, KL: 84.85, Word: 30.15, Topo: 79.03, Assm: 71.70, Clsf: 91.67\n",
      "Epoch: 0 | Iter: 85\n",
      "[Train] Loss: 146.608, Clsf_loss: 50.34, KL: 101.43, Word: 32.64, Topo: 80.90, Assm: 70.02, Clsf: 82.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 66, in <module>\n",
      "    if len(supervised_batch['labels']) == batch_size and len(unsupervised_batch['labels']) == batch_size:\n",
      "TypeError: 'NoneType' object has no attribute '__getitem__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iter: 90\n",
      "[Train] Loss: 155.279, Clsf_loss: 50.50, KL: 107.74, Word: 29.05, Topo: 80.20, Assm: 68.63, Clsf: 85.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 66, in <module>\n",
      "    if len(supervised_batch['labels']) == batch_size and len(unsupervised_batch['labels']) == batch_size:\n",
      "TypeError: 'NoneType' object has no attribute '__getitem__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iter: 95\n",
      "[Train] Loss: 139.767, Clsf_loss: 49.38, KL: 103.37, Word: 29.93, Topo: 82.18, Assm: 70.50, Clsf: 85.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 66, in <module>\n",
      "    if len(supervised_batch['labels']) == batch_size and len(unsupervised_batch['labels']) == batch_size:\n",
      "TypeError: 'NoneType' object has no attribute '__getitem__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to data/tox21/model/model.iter-at step: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 93, in <module>\n",
      "    torch.save(model.state_dict(), save_dir + \"/model.iter-\" + str(total_step))\n",
      "  File \"/home/tsa87/miniconda3/envs/python2/lib/python2.7/site-packages/torch/serialization.py\", line 260, in save\n",
      "    return _with_file_like(f, \"wb\", lambda f: _save(obj, f, pickle_module, pickle_protocol))\n",
      "  File \"/home/tsa87/miniconda3/envs/python2/lib/python2.7/site-packages/torch/serialization.py\", line 183, in _with_file_like\n",
      "    f = open(f, mode)\n",
      "IOError: [Errno 2] No such file or directory: 'data/tox21/model/model.iter-100'\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 66, in <module>\n",
      "    if len(supervised_batch['labels']) == batch_size and len(unsupervised_batch['labels']) == batch_size:\n",
      "TypeError: 'NoneType' object has no attribute '__getitem__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iter: 105\n",
      "[Train] Loss: 137.298, Clsf_loss: 47.73, KL: 108.92, Word: 29.75, Topo: 80.98, Assm: 68.29, Clsf: 88.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 66, in <module>\n",
      "    if len(supervised_batch['labels']) == batch_size and len(unsupervised_batch['labels']) == batch_size:\n",
      "TypeError: 'NoneType' object has no attribute '__getitem__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iter: 110\n",
      "[Train] Loss: 125.618, Clsf_loss: 44.40, KL: 104.92, Word: 31.83, Topo: 84.88, Assm: 76.60, Clsf: 90.00\n",
      "Epoch: 0 | Iter: 115\n",
      "[Train] Loss: 126.708, Clsf_loss: 42.98, KL: 114.07, Word: 31.72, Topo: 84.01, Assm: 68.92, Clsf: 92.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 66, in <module>\n",
      "    if len(supervised_batch['labels']) == batch_size and len(unsupervised_batch['labels']) == batch_size:\n",
      "TypeError: 'NoneType' object has no attribute '__getitem__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 0.000729\n",
      "Epoch: 0 | Iter: 120\n",
      "[Train] Loss: 133.173, Clsf_loss: 45.43, KL: 122.60, Word: 31.42, Topo: 83.55, Assm: 74.82, Clsf: 87.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 66, in <module>\n",
      "    if len(supervised_batch['labels']) == batch_size and len(unsupervised_batch['labels']) == batch_size:\n",
      "TypeError: 'NoneType' object has no attribute '__getitem__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iter: 125\n",
      "[Train] Loss: 122.145, Clsf_loss: 43.19, KL: 123.60, Word: 32.55, Topo: 83.31, Assm: 81.00, Clsf: 90.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 66, in <module>\n",
      "    if len(supervised_batch['labels']) == batch_size and len(unsupervised_batch['labels']) == batch_size:\n",
      "TypeError: 'NoneType' object has no attribute '__getitem__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iter: 130\n",
      "[Train] Loss: 135.573, Clsf_loss: 42.90, KL: 141.11, Word: 29.22, Topo: 85.06, Assm: 71.51, Clsf: 92.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 66, in <module>\n",
      "    if len(supervised_batch['labels']) == batch_size and len(unsupervised_batch['labels']) == batch_size:\n",
      "TypeError: 'NoneType' object has no attribute '__getitem__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iter: 135\n",
      "[Train] Loss: 129.896, Clsf_loss: 43.51, KL: 130.14, Word: 31.63, Topo: 84.82, Assm: 76.57, Clsf: 92.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 66, in <module>\n",
      "    if len(supervised_batch['labels']) == batch_size and len(unsupervised_batch['labels']) == batch_size:\n",
      "TypeError: 'NoneType' object has no attribute '__getitem__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iter: 140\n",
      "[Train] Loss: 122.580, Clsf_loss: 39.50, KL: 125.90, Word: 31.87, Topo: 85.73, Assm: 77.69, Clsf: 95.00\n",
      "Epoch: 0 | Iter: 145\n",
      "[Train] Loss: 120.767, Clsf_loss: 37.93, KL: 123.42, Word: 30.26, Topo: 85.11, Assm: 71.25, Clsf: 97.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 66, in <module>\n",
      "    if len(supervised_batch['labels']) == batch_size and len(unsupervised_batch['labels']) == batch_size:\n",
      "TypeError: 'NoneType' object has no attribute '__getitem__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to data/tox21/model/model.iter-at step: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 93, in <module>\n",
      "    torch.save(model.state_dict(), save_dir + \"/model.iter-\" + str(total_step))\n",
      "  File \"/home/tsa87/miniconda3/envs/python2/lib/python2.7/site-packages/torch/serialization.py\", line 260, in save\n",
      "    return _with_file_like(f, \"wb\", lambda f: _save(obj, f, pickle_module, pickle_protocol))\n",
      "  File \"/home/tsa87/miniconda3/envs/python2/lib/python2.7/site-packages/torch/serialization.py\", line 183, in _with_file_like\n",
      "    f = open(f, mode)\n",
      "IOError: [Errno 2] No such file or directory: 'data/tox21/model/model.iter-150'\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 66, in <module>\n",
      "    if len(supervised_batch['labels']) == batch_size and len(unsupervised_batch['labels']) == batch_size:\n",
      "TypeError: 'NoneType' object has no attribute '__getitem__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iter: 155\n",
      "[Train] Loss: 125.857, Clsf_loss: 39.65, KL: 125.54, Word: 31.35, Topo: 84.67, Assm: 73.01, Clsf: 95.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-997b423b499f>\", line 66, in <module>\n",
      "    if len(supervised_batch['labels']) == batch_size and len(unsupervised_batch['labels']) == batch_size:\n",
      "TypeError: 'NoneType' object has no attribute '__getitem__'\n"
     ]
    }
   ],
   "source": [
    "total_step = load_epoch\n",
    "beta = beta\n",
    "\n",
    "train_meter_hist = []\n",
    "val_meter_hist = []\n",
    "\n",
    "def numpy_label_to_onehot_tensor(np_labels):    \n",
    "    labels = torch.from_numpy(np_labels)[:, None].long()\n",
    "    labels = torch.zeros(batch_size, 2).scatter_(1, labels, 1)\n",
    "    labels = labels.cuda() if has_cuda else labels\n",
    "    return labels\n",
    "\n",
    "start = time.time()\n",
    "for epoch in xrange(epoch):\n",
    "    \n",
    "    meters = np.zeros(8) # loss, kl_div, wacc, tacc, sacc, clsf_acc, division_factor\n",
    "    val_meter_hist.append(np.zeros(8))\n",
    "    train_meter_hist.append(np.zeros(8))\n",
    "\n",
    "    preds = np.array([])\n",
    "    targets = np.array([])\n",
    "    \n",
    "    # Evaluation loop\n",
    "    if epoch % test_epoch == 0 and epoch > 0:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "        \n",
    "            test_loader = MolTreeFolder(test_folder, vocab, batch_size=batch_size, label_idx=label_col, num_workers=num_workers)\n",
    "\n",
    "            for (supervised_batch, unsupervised_batch) in test_loader:  \n",
    "                try:\n",
    "                    if len(supervised_batch['labels']) == batch_size:\n",
    "                    \n",
    "                        supervised_input = supervised_batch['data']\n",
    "                        labels = numpy_label_to_onehot_tensor(supervised_batch['labels'])\n",
    "                        \n",
    "                        loss, clsf_loss, kl_div, wacc, tacc, sacc, clsf_acc, (pred, target) = model(supervised_input, labels, beta)\n",
    "                        \n",
    "                        preds = np.append(preds, pred.cpu().detach().numpy())\n",
    "                        targets = np.append(targets, target.cpu().detach().numpy())\n",
    "\n",
    "                        meters = meters + np.array([loss, clsf_loss, kl_div, wacc * 100, tacc * 100, sacc * 100, clsf_acc * 100, 1.], dtype=np.float32)\n",
    "\n",
    "                except Exception as e:\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "\n",
    "            print(classification_report(targets, preds)) \n",
    "            \n",
    "            if meters[-1] > 0:\n",
    "                meters /= meters[-1]\n",
    "                print 'time: %s' % time_since(start)\n",
    "                print \"[Test] Loss: %.3f, Clsf_loss: %.2f, KL: %.2f, Word: %.2f, Topo: %.2f, Assm: %.2f, Clsf: %.2f\" % (loss, meters[1], meters[2], meters[3], meters[4], meters[5], meters[6])\n",
    "                val_meter_hist[-1] = meters\n",
    "                meters *= 0\n",
    "            else:\n",
    "                val_meter_hist[-1] = np.full_like(val_meter_hist[-1], np.nan, dtype=np.double)\n",
    "                    \n",
    "                \n",
    "    train_loader = MolTreeFolder(train_folder, vocab, batch_size=batch_size, label_idx=label_col, num_workers=num_workers, index_tracker=tracker)    \n",
    "    meters *= 0\n",
    "    model.train()\n",
    "    \n",
    "    for (supervised_batch, unsupervised_batch) in train_loader: \n",
    "        try:\n",
    "            if len(supervised_batch['labels']) == batch_size and len(unsupervised_batch['labels']) == batch_size:\n",
    "                model.zero_grad()\n",
    "                \n",
    "                total_step += 1\n",
    "                labels = numpy_label_to_onehot_tensor(supervised_batch['labels'])\n",
    "                \n",
    "                unsupervised_loss, _, kl_div1, wacc1, tacc1, sacc1, _, _ = model(unsupervised_batch['data'], None, beta)\n",
    "                supervised_loss, clsf_loss, kl_div2, wacc2, tacc2, sacc2, clsf_acc, (pred, target) = model(supervised_batch['data'], labels, beta)\n",
    "\n",
    "                # print(classification_report(target.cpu().detach().numpy(), pred.cpu().detach().numpy())) \n",
    "                \n",
    "                loss = unsupervised_loss + supervised_loss\n",
    "\n",
    "                kl_div = kl_div1 + kl_div2\n",
    "                wacc = (wacc1 + wacc2)/2\n",
    "                tacc = (tacc1 + tacc2)/2\n",
    "                sacc = (sacc1 + sacc2)/2\n",
    "\n",
    "                meters = meters + np.array([loss, clsf_loss, kl_div, wacc * 100, tacc * 100, sacc * 100, clsf_acc * 100, 1], dtype=np.float32)\n",
    "\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if total_step % save_iter == 0:\n",
    "                    print \"Saving model to \" + save_dir + \"/model.iter-\" + \"at step: \" + str(total_step)\n",
    "                    torch.save(model.state_dict(), save_dir + \"/model.iter-\" + str(total_step))\n",
    "\n",
    "                if total_step % anneal_iter == 0:\n",
    "                    scheduler.step()\n",
    "                    print \"learning rate: %.6f\" % scheduler.get_lr()[0]\n",
    "\n",
    "                if total_step % kl_anneal_iter == 0 and total_step >= warmup:\n",
    "                    beta = min(max_beta, beta + step_beta)\n",
    "\n",
    "                if total_step % print_iter == 0:\n",
    "                    meters /= meters[-1]\n",
    "                    \n",
    "                    print \"Epoch: %d | Iter: %d\" % (epoch, total_step)\n",
    "                    print \"[Train] Loss: %.3f, Clsf_loss: %.2f, KL: %.2f, Word: %.2f, Topo: %.2f, Assm: %.2f, Clsf: %.2f\" % (meters[0], meters[1], meters[2], meters[3], meters[4], meters[5], meters[6])\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "                    # Cache metrics for plotting\n",
    "                    train_meter_hist[-1] += meters\n",
    "                    meters *= 0\n",
    "\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "            \n",
    "    scheduler.step()\n",
    "    train_meter_hist[-1] = train_meter_hist[-1] / train_meter_hist[-1][-1]\n",
    "    \n",
    "    improved = True\n",
    "    if len(train_meter_hist) > early_stop_thresh+1:\n",
    "        improved = False\n",
    "        baseline_loss = train_meter_hist[early_stop_thresh*-1-1][0]\n",
    "        for i in range(early_stop_thresh):\n",
    "            if baseline_loss > train_meter_hist[i*-1][0]:\n",
    "                improved = True\n",
    "                \n",
    "    if not improved:\n",
    "        print(\"Stopping early due to no improvement\")\n",
    "        print(classification_report(actuals, preds))\n",
    "        break\n",
    "    else:\n",
    "        continue\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Data for plotting\n",
    "# t = np.arange(len(train_meter_hist))\n",
    "\n",
    "# plt.plot(t, np.array(train_meter_hist)[:, 0], label='train loss')\n",
    "# plt.plot(t, np.array(val_meter_hist)[:, 0], label='validation loss')\n",
    "\n",
    "# plt.title('Loss vs Epoch')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
